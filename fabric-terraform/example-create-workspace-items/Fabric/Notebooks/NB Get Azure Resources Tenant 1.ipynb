{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07225bdf-d530-4fed-bc59-5aef48e14926",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# NB Get Azure Resources Tenant - Notebooks Summary\n",
    "\n",
    "## Overview\n",
    "These notebooks (Tenant 1 through Tenant 8) are comprehensive Azure data extraction and processing tools designed to collect cloud resource consumption, licensing, and infrastructure metadata from multiple Azure tenants. Each tenant notebook operates independently to extract data from its respective Azure subscription and tenant.\n",
    "\n",
    "## Key Functionalities\n",
    "\n",
    "### 1. **Authentication & Authorization**\n",
    "- Retrieves credentials from Azure Key Vault (Client ID, Client Secret, Tenant ID)\n",
    "- Obtains OAuth 2.0 Bearer tokens for both Azure Management and Microsoft Graph APIs\n",
    "- Supports multi-tenant authentication\n",
    "\n",
    "### 2. **Azure Subscriptions & Resources**\n",
    "- **Get Azure Subscriptions**: Fetches all subscriptions associated with the tenant\n",
    "- Creates `StagingdimAzSubscriptions` Delta table with subscription metadata (ID, name, state, quota type)\n",
    "\n",
    "### 3. **Azure Consumption Data**\n",
    "- **Actual Cost & Amortized Cost Reports**: \n",
    "  - Extracts cost consumption data using Azure Cost Management API\n",
    "  - Supports configurable time periods (current month, last month, or custom lookback periods)\n",
    "  - Downloads cost reports as CSV files to lakehouse storage\n",
    "  - Implements concurrent processing (ThreadPoolExecutor) for efficient multi-subscription handling\n",
    "  - Handles rate limiting and API retry logic with exponential backoff\n",
    "\n",
    "### 4. **Azure Reservations**\n",
    "- Retrieves all Azure Reserved Instance details\n",
    "- Extracts utilization metrics (current usage, trend, grain metrics)\n",
    "- Flattens nested JSON data for CSV export\n",
    "- Captures billing plans and scope properties\n",
    "\n",
    "### 5. **Azure Saving Plans**\n",
    "- Fetches Azure Savings Plan commitment details\n",
    "- Extracts commitment amounts, currency, and utilization data\n",
    "- Handles missing/null fields with default values\n",
    "- Exports comprehensive schema to CSV\n",
    "\n",
    "### 6. **Azure Tags & Metadata**\n",
    "- **Resource Group Tags**: Extracts tags from all resource groups across subscriptions\n",
    "- **Subscription Tags**: Collects subscription-level tags\n",
    "- Uses multi-threaded approach for efficient processing\n",
    "- Implements retry logic for API resilience\n",
    "\n",
    "### 7. **Microsoft 365 Licensing**\n",
    "- **Tenant Licenses**: Queries Microsoft Graph API for M365 SKU data\n",
    "  - Available, consumed, and remaining license counts\n",
    "  - Stores in `StagingdimAzMSTenantLicences` table\n",
    "\n",
    "- **User Licenses & Overlap Detection**:\n",
    "  - Retrieves detailed user license assignments\n",
    "  - Detects over-licensing scenarios (e.g., user with both E3 and E5)\n",
    "  - Captures user metadata (name, UPN, department, company)\n",
    "  - Stores in `StagingdimAzMSUserLicenses` table\n",
    "\n",
    "## Data Storage\n",
    "\n",
    "**Lakehouse**: lakehouse01\n",
    "**Tables Created**:\n",
    "- `StagingdimAzSubscriptions` - Subscription metadata\n",
    "- `StagingdimAzMSTenantLicences` - Tenant-level license summary\n",
    "- `StagingdimAzMSUserLicenses` - User-level license details with overlap detection\n",
    "\n",
    "**File Paths**:\n",
    "- `/Files/azure-Usages/actualcost/` - Cost consumption files\n",
    "- `/Files/azure-Usages/amortizedcost/` - Amortized cost files\n",
    "- `/Files/azure/reservations/` - Reservation details\n",
    "- `/Files/azure/saving-plans/` - Saving plan details\n",
    "- `/Files/azure/resource-import-tags/` - Resource group tags\n",
    "- `/Files/azure/subscription-tags/` - Subscription tags\n",
    "\n",
    "## Technical Details\n",
    "\n",
    "**Runtime Environment**: Synapse PySpark\n",
    "**Key Libraries**:\n",
    "- `requests` - Azure API calls\n",
    "- `pandas` - Data manipulation\n",
    "- `pyspark.sql` - Distributed processing\n",
    "- `mssparkutils` - Fabric integration\n",
    "- `concurrent.futures` - Multi-threading\n",
    "\n",
    "**Error Handling**:\n",
    "- Rate limit (429) retry logic with configurable backoff\n",
    "- Request timeout handling\n",
    "- Missing/null value defaults\n",
    "- Comprehensive logging and error reporting\n",
    "\n",
    "## Execution Flow\n",
    "\n",
    "1. Load workspace variables and credentials\n",
    "2. Authenticate to Azure Management and Graph APIs\n",
    "3. Retrieve subscriptions for the tenant\n",
    "4. Extract consumption data (cost metrics)\n",
    "5. Fetch reservations and saving plans\n",
    "6. Collect tags at resource and subscription level\n",
    "7. Query M365 licensing information\n",
    "8. Detect license overlaps and over-licensing\n",
    "9. Write all data to Delta tables and CSV files for downstream ETL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42010c-5834-45c1-8a24-f4fec3cbc1bd",
   "metadata": {},
   "source": [
    "# Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608b154-4c46-46ce-82e3-a2696578b4fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-02T14:40:43.4948379Z",
       "execution_start_time": "2026-02-02T14:40:42.528489Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "f321e66e-1109-4db1-9b73-9c7c8fd43dca",
       "queued_time": "2026-02-02T14:40:42.5274532Z",
       "session_id": "564724c6-dbe5-4a67-9f18-ef0800e8f833",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 27,
       "statement_ids": [
        27
       ]
      },
      "text/plain": [
       "StatementMeta(, 564724c6-dbe5-4a67-9f18-ef0800e8f833, 27, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import calendar\n",
    "import hashlib\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Third-party libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from msal import ConfidentialClientApplication\n",
    "import threading\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, when, upper\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454dc7e6-c40d-46cd-b424-91e31a9aa474",
   "metadata": {},
   "source": [
    "# Import workspace variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abb7f386-545b-4835-aa48-d1ec40d6c949",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-02T14:40:51.457004Z",
       "execution_start_time": "2026-02-02T14:40:43.4968683Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "65f1f69a-a244-4737-ac9a-64d306b490b0",
       "queued_time": "2026-02-02T14:40:42.5833591Z",
       "session_id": "564724c6-dbe5-4a67-9f18-ef0800e8f833",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 28,
       "statement_ids": [
        28
       ]
      },
      "text/plain": [
       "StatementMeta(, 564724c6-dbe5-4a67-9f18-ef0800e8f833, 28, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = notebookutils.variableLibrary.getLibrary(\"ws_variables\")\n",
    "\n",
    "key_vault_name = cfg.key_vault_name\n",
    "kvtenantid_1 = cfg.kvtenantid_1\n",
    "kvclientid_1 = cfg.kvclientid_1\n",
    "kvclientkey_1 = cfg.kvclientkey_1\n",
    "sku_mapping_path = cfg.sku_mapping_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc39cb-dbdd-4713-9910-566a079503ed",
   "metadata": {},
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd54eed7-9383-4117-a2bd-99f9af92393c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-02T14:40:51.8337053Z",
       "execution_start_time": "2026-02-02T14:40:51.4590722Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "46e3ecf5-15fe-4666-b982-281f9ae7697e",
       "queued_time": "2026-02-02T14:40:42.7564633Z",
       "session_id": "564724c6-dbe5-4a67-9f18-ef0800e8f833",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 29,
       "statement_ids": [
        29
       ]
      },
      "text/plain": [
       "StatementMeta(, 564724c6-dbe5-4a67-9f18-ef0800e8f833, 29, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LakehousePathActualCost = \"/lakehouse/default/Files/azure-Usages/actualcost\"\n",
    "LakehousePathAmortizedCost = \"/lakehouse/default/Files/azure-Usages/amortizedcost\"\n",
    "ReservationPath = \"/lakehouse/default/Files/azure/reservations\"\n",
    "SavingPlansPath = \"/lakehouse/default/Files/azure/saving-plans\"\n",
    "LakehouseSubscriptionsPath = \"lakehouse01.StagingdimAzSubscriptions\"\n",
    "ResourceTagsPath = \"/lakehouse/default/Files/azure/resource-import-tags\"\n",
    "SubscriptionTagsPath = \"/lakehouse/default/Files/azure/subscription-tags\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b63dbf-32d6-4e97-96be-fc9f2ddbcc0f",
   "metadata": {},
   "source": [
    "# Get Azure Key Vault Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6118d607-b6c4-4b6a-98dd-b941c0098eed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-02T14:40:52.3525704Z",
       "execution_start_time": "2026-02-02T14:40:51.8356365Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "85dd4922-3ae5-42f1-9362-1ab91d7c0110",
       "queued_time": "2026-02-02T14:40:42.7590697Z",
       "session_id": "564724c6-dbe5-4a67-9f18-ef0800e8f833",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 30,
       "statement_ids": [
        30
       ]
      },
      "text/plain": [
       "StatementMeta(, 564724c6-dbe5-4a67-9f18-ef0800e8f833, 30, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app_client_id = mssparkutils.credentials.getSecret(f\"https://{key_vault_name}.vault.azure.net/\", kvclientid_1)\n",
    "\n",
    "app_client_secret = mssparkutils.credentials.getSecret(f\"https://{key_vault_name}.vault.azure.net/\", kvclientkey_1)\n",
    "\n",
    "microsoft_tenant_id = mssparkutils.credentials.getSecret(f\"https://{key_vault_name}.vault.azure.net/\", kvtenantid_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2a21a-98c1-49ee-9ba8-49e794e54aee",
   "metadata": {},
   "source": [
    "# Get Access Token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a3d99-c76d-4cf5-a805-6aa933f10987",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-02T14:40:52.6927639Z",
       "execution_start_time": "2026-02-02T14:40:52.3547076Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b0e7918f-07f1-4f12-b7a1-97160709b066",
       "queued_time": "2026-02-02T14:40:42.776785Z",
       "session_id": "564724c6-dbe5-4a67-9f18-ef0800e8f833",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 31,
       "statement_ids": [
        31
       ]
      },
      "text/plain": [
       "StatementMeta(, 564724c6-dbe5-4a67-9f18-ef0800e8f833, 31, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOKEN_CACHE = {\n",
    "    \"access_token\": None,\n",
    "    \"expires_at\": 0\n",
    "}\n",
    "\n",
    "TOKEN_LOCK = threading.Lock()\n",
    "\n",
    "def get_access_token():\n",
    "    now = time.time()\n",
    "\n",
    "    # Token still valid (5 min buffer)\n",
    "    if TOKEN_CACHE[\"access_token\"] and now < TOKEN_CACHE[\"expires_at\"] - 300:\n",
    "        return TOKEN_CACHE[\"access_token\"]\n",
    "\n",
    "    with TOKEN_LOCK:\n",
    "        if TOKEN_CACHE[\"access_token\"] and now < TOKEN_CACHE[\"expires_at\"] - 300:\n",
    "            return TOKEN_CACHE[\"access_token\"]\n",
    "\n",
    "        uri = f\"https://login.microsoftonline.com/{microsoft_tenant_id}/oauth2/token\"\n",
    "\n",
    "        data = {\n",
    "            \"client_id\": app_client_id,\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_secret\": app_client_secret,\n",
    "            \"resource\": \"https://management.core.windows.net\"\n",
    "        }\n",
    "\n",
    "        response = requests.post(uri, data=data)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        token_response = response.json()\n",
    "        TOKEN_CACHE[\"access_token\"] = token_response[\"access_token\"]\n",
    "        TOKEN_CACHE[\"expires_at\"] = now + int(token_response[\"expires_in\"])\n",
    "\n",
    "        return TOKEN_CACHE[\"access_token\"]\n",
    "\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {get_access_token()}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a9d278-e6a0-4216-b726-fd5ea94d7fc5",
   "metadata": {},
   "source": [
    "# Get Azure Subscriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23866b7b-69c0-4117-9e01-3a7d83962025",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# The API URL\n",
    "url = \"https://management.azure.com/subscriptions?api-version=2022-12-01\"\n",
    "\n",
    "# Send API request\n",
    "response = requests.get(url, headers=get_headers())\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    if 'value' in data:\n",
    "        df_pandas = pd.json_normalize(data['value'])\n",
    "    else:\n",
    "        df_pandas = pd.DataFrame(data)\n",
    "\n",
    "    required_columns = [\n",
    "        'id',\n",
    "        'subscriptionId',\n",
    "        'tenantId',\n",
    "        'displayName',\n",
    "        'state',\n",
    "        'subscriptionPolicies.quotaId'\n",
    "    ]\n",
    "    df_pandas = df_pandas[required_columns]\n",
    "\n",
    "    df_pandas.columns = ['value.' + col for col in df_pandas.columns]\n",
    "\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "    print(\"Data successfully loaded into PySpark DataFrame.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8d8621-b80f-405c-b4a0-d787471a96ee",
   "metadata": {},
   "source": [
    "# Write Delta Table StagingdimAzSubscriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a0348-325d-4253-b625-c9142a651bdc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Path to Delta Table\n",
    "delta_table_path_stagingdimazsubscriptions = \"Tables/StagingdimAzSubscriptions\"\n",
    "\n",
    "# Write to Delta Table\n",
    "df_spark.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_stagingdimazsubscriptions)\n",
    "\n",
    "print(\"Data successfully written to Delta Table.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88530b46-1b69-42e1-b2f9-21f17b32fd42",
   "metadata": {},
   "source": [
    "# Get Azure Consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f3100-ba82-4a2e-b5cc-2d0e6d701069",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Helper function: Determine time periods\n",
    "# -------------------------------------------------\n",
    "def get_month_periods(months_back=None):\n",
    "    today = datetime.now()\n",
    "    periods = []\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # OPTIONAL: Last X months\n",
    "    # ---------------------------------------------\n",
    "    if months_back is not None:\n",
    "        for i in range(months_back):\n",
    "            ref_date = today.replace(day=1) - DateOffset(months=i)\n",
    "            start_date = ref_date.to_pydatetime()\n",
    "\n",
    "            last_day = calendar.monthrange(start_date.year, start_date.month)[1]\n",
    "\n",
    "            # Current month only until today\n",
    "            if i == 0:\n",
    "                end_date = today\n",
    "            else:\n",
    "                end_date = start_date.replace(day=last_day)\n",
    "\n",
    "            periods.append((start_date, end_date))\n",
    "\n",
    "        return periods\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # STANDARD: Previous behavior\n",
    "    # ---------------------------------------------\n",
    "    start_current = today.replace(day=1)\n",
    "    end_current = today\n",
    "    periods.append((start_current, end_current))\n",
    "\n",
    "    if 1 <= today.day <= 6:\n",
    "        last_month_date = start_current - timedelta(days=1)\n",
    "        start_last_month = last_month_date.replace(day=1)\n",
    "        last_day_of_last_month = calendar.monthrange(\n",
    "            start_last_month.year, start_last_month.month\n",
    "        )[1]\n",
    "        end_last_month = start_last_month.replace(day=last_day_of_last_month)\n",
    "        periods.append((start_last_month, end_last_month))\n",
    "\n",
    "    return periods\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Report function\n",
    "# -------------------------------------------------\n",
    "def fetch_report_data(subscriptionid, TenantId, Vertragsart, metric_type, file_path, months_back=None):\n",
    "\n",
    "    months = get_month_periods(months_back)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Pull report for all time periods\n",
    "    # -------------------------------------------------\n",
    "    for start_date, end_date in months:\n",
    "        start_str = start_date.strftime('%Y-%m-%d')\n",
    "        end_str = end_date.strftime('%Y-%m-%d')\n",
    "        ym_str = start_date.strftime(\"%Y-%m\")\n",
    "\n",
    "        body = {\n",
    "            \"metric\": metric_type,\n",
    "            \"timePeriod\": {\n",
    "                \"start\": start_str,\n",
    "                \"end\": end_str\n",
    "            }\n",
    "        }\n",
    "\n",
    "        url = (\n",
    "            f\"https://management.azure.com/subscriptions/{subscriptionid}\"\n",
    "            f\"/providers/Microsoft.CostManagement/generateCostDetailsReport\"\n",
    "            f\"?api-version=2022-05-01\"\n",
    "        )\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Storage path (always complete month)\n",
    "        # -------------------------------------------------\n",
    "        first_day_of_month = start_date.replace(day=1)\n",
    "        last_day_num = calendar.monthrange(start_date.year, start_date.month)[1]\n",
    "        last_day_of_month = start_date.replace(day=last_day_num)\n",
    "\n",
    "        period_str = f\"{first_day_of_month.strftime('%Y%m%d')}-{last_day_of_month.strftime('%Y%m%d')}\"\n",
    "        date_path = f\"{file_path}/{period_str}\"\n",
    "        os.makedirs(date_path, exist_ok=True)\n",
    "\n",
    "        filename = f\"{TenantId}-{Vertragsart}-{subscriptionid}-{ym_str}.csv\"\n",
    "        path = f\"{date_path}/{filename}\"\n",
    "\n",
    "\n",
    "        # =================================================\n",
    "        # ðŸ”¹ HERE: Skip month if already exists\n",
    "        # (ONLY if months_back is set)\n",
    "        # =================================================\n",
    "        if months_back is not None and os.path.exists(path):\n",
    "            print(f\"Skipped (already exists): {path}\")\n",
    "            continue\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Step 1: Create report\n",
    "        # -------------------------------------------------\n",
    "        while True:\n",
    "            response = requests.post(url, headers=get_headers(), json=body)\n",
    "\n",
    "            if response.status_code == 204:\n",
    "                print(f\"No content: {subscriptionid} ({start_str} to {end_str})\")\n",
    "                break\n",
    "\n",
    "            elif response.status_code == 202:\n",
    "                break\n",
    "\n",
    "            elif response.status_code == 429:\n",
    "                retry_after = response.headers.get(\"Retry-After\")\n",
    "                wait_time = int(retry_after) if retry_after and retry_after.isdigit() else 30\n",
    "\n",
    "                if wait_time > 60:\n",
    "                    print(f\"Rate limit â€“ Subscription skipped: {subscriptionid}\")\n",
    "                    return\n",
    "\n",
    "                print(f\"Rate limit â€“ waiting {wait_time}s\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "            else:\n",
    "                raise Exception(f\"Error creating report: {response.status_code}\")\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Step 2: Retrieve & save report\n",
    "        # -------------------------------------------------\n",
    "        attempts = 0\n",
    "        while attempts < 12:\n",
    "            check_response = requests.get(response.headers[\"location\"], headers=get_headers())\n",
    "\n",
    "            if check_response.status_code == 200:\n",
    "                response_dict = check_response.json()\n",
    "\n",
    "                if 'manifest' in response_dict and 'blobs' in response_dict['manifest']:\n",
    "                    blob_link = response_dict['manifest']['blobs'][0]['blobLink']\n",
    "                    csv_df = pd.read_csv(blob_link, low_memory=False)\n",
    "\n",
    "                    if not csv_df.empty:\n",
    "                        csv_df.to_csv(path, index=False)\n",
    "                        print(f\"Saved: {path}\")\n",
    "                    else:\n",
    "                        print(f\"No data: {subscriptionid} ({start_str} to {end_str})\")\n",
    "                else:\n",
    "                    raise Exception(\"No blob link in response\")\n",
    "\n",
    "                break\n",
    "\n",
    "            elif check_response.status_code == 204:\n",
    "                print(f\"No content to retrieve: {subscriptionid}\")\n",
    "                break\n",
    "\n",
    "            elif check_response.status_code == 429:\n",
    "                retry_after = check_response.headers.get(\"Retry-After\")\n",
    "                wait_time = int(retry_after) if retry_after and retry_after.isdigit() else 30\n",
    "\n",
    "                if wait_time > 60:\n",
    "                    print(f\"Rate limit â€“ skipping {subscriptionid}\")\n",
    "                    return\n",
    "\n",
    "                time.sleep(wait_time)\n",
    "                attempts += 1\n",
    "\n",
    "            elif check_response.status_code == 202:\n",
    "                attempts += 1\n",
    "                time.sleep(30)\n",
    "\n",
    "            else:\n",
    "                raise Exception(f\"Error retrieving: {check_response.status_code}\")\n",
    "\n",
    "        if attempts >= 8:\n",
    "            print(f\"Maximum attempts reached: {subscriptionid}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Runner\n",
    "# -------------------------------------------------\n",
    "def run_script(metric_type, file_path, months_back=None):\n",
    "    spark = SparkSession.builder.appName(\"CostReport\").getOrCreate()\n",
    "\n",
    "    SubscriptionId_df = spark.sql(\n",
    "        f\"\"\"\n",
    "        SELECT `value.subscriptionId` as SubscriptionId,\n",
    "               `value.tenantId` as CustomerTenantId,\n",
    "               `value.subscriptionPolicies.quotaId` as Vertragsart\n",
    "        FROM {LakehouseSubscriptionsPath}\n",
    "        WHERE `value.tenantId` = '{microsoft_tenant_id}'\n",
    "          AND `value.subscriptionPolicies.quotaId` LIKE 'CSP%'\n",
    "          OR `value.subscriptionPolicies.quotaId`LIKE 'Pay%'\n",
    "          OR `value.subscriptionPolicies.quotaId`LIKE 'Enterprise%'\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    SubscriptionId_list = [\n",
    "        [row[\"SubscriptionId\"], row[\"CustomerTenantId\"], row[\"Vertragsart\"]]\n",
    "        for row in SubscriptionId_df.collect()\n",
    "    ]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_subscription = {\n",
    "            executor.submit(\n",
    "                fetch_report_data,\n",
    "                subscriptionid,\n",
    "                TenantId,\n",
    "                Vertragsart,\n",
    "                metric_type,\n",
    "                file_path,\n",
    "                months_back\n",
    "            ): subscriptionid\n",
    "            for subscriptionid, TenantId, Vertragsart in SubscriptionId_list\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_subscription):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Error with {future_to_subscription[future]}: {exc}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Main calls\n",
    "# -------------------------------------------------\n",
    "\n",
    "# ðŸ”¹ Standard (as before)\n",
    "run_script(\"ActualCost\", LakehousePathActualCost)\n",
    "run_script(\"AmortizedCost\", LakehousePathAmortizedCost)\n",
    "\n",
    "# ðŸ”¹ Optional (e.g., last 6 months)\n",
    "#run_script(\"ActualCost\", LakehousePathActualCost, months_back=12)\n",
    "#run_script(\"AmortizedCost\", LakehousePathAmortizedCost, months_back=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c899e-2922-422c-8088-190c3d376f46",
   "metadata": {},
   "source": [
    "# Get Azure Reservations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50951c-5714-4895-8a20-46812f574b82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# API version for Azure Resource Manager\n",
    "api_version = '2022-11-01'  # Alternative: try '2021-10-01'\n",
    "\n",
    "# Header with Bearer Token\n",
    "#headers = {\n",
    "    #'Authorization': f'Bearer {token}',\n",
    "    #'Content-Type': 'application/json'\n",
    "#}\n",
    "\n",
    "# Target path for CSV file\n",
    "destination_path = ReservationPath\n",
    "csv_filename = os.path.join(destination_path, f'Reservations-{microsoft_tenant_id}.csv')\n",
    "\n",
    "# Check if target directory exists, if not, create it\n",
    "if not os.path.exists(destination_path):\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "os.chdir(destination_path)\n",
    "print(os.getcwd())\n",
    "# URL for Azure Reservations\n",
    "reservations_url = f'https://management.azure.com/providers/Microsoft.Capacity/reservations?api-version={api_version}'\n",
    "\n",
    "# Function to flatten nested JSON objects\n",
    "def flatten_json(nested_json, parent_key='', sep='.'):\n",
    "    flattened = {}\n",
    "    for key, value in nested_json.items():\n",
    "        new_key = f'{parent_key}{sep}{key}' if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, sep=sep))\n",
    "        elif isinstance(value, list):\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    flattened.update(flatten_json(item, f'{new_key}[{i}]', sep=sep))\n",
    "                else:\n",
    "                    flattened[f'{new_key}[{i}]'] = item\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "# Function to fetch all pages with pagination\n",
    "def fetch_all_reservations_with_pagination(initial_url):\n",
    "    all_data = []\n",
    "    current_url = initial_url\n",
    "    while current_url:\n",
    "        response = requests.get(current_url, headers=get_headers()) # NEW get_headers() #\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if 'value' in result:\n",
    "                all_data.extend(result['value'])\n",
    "            current_url = result.get('nextLink')  # Get next link if available\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}, Message: {response.text}\")\n",
    "            current_url = None  # Stop on error\n",
    "    return all_data\n",
    "\n",
    "# Fetch reservations and write directly to CSV file\n",
    "try:\n",
    "    reservations = fetch_all_reservations_with_pagination(reservations_url)\n",
    "    \n",
    "    print(\"Status code:\", 200)  # Status code for successful request\n",
    "    if reservations:\n",
    "        print(\"Reservations successfully retrieved.\")\n",
    "        \n",
    "        # Extract data and write directly to CSV file\n",
    "        data = reservations\n",
    "        \n",
    "        # Check if there are reservations\n",
    "        if data:\n",
    "            # Prepare flat list\n",
    "            flattened_data = [flatten_json(item) for item in data]\n",
    "            \n",
    "            # Add Filename column\n",
    "            for item in flattened_data:\n",
    "                item['Filename'] = os.path.basename(csv_filename)  # Only filename without path\n",
    "            \n",
    "            # Create CSV file and write data\n",
    "            with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "                # CSV header: all unique keys from flat data\n",
    "                fieldnames = set()\n",
    "                for item in flattened_data:\n",
    "                    fieldnames.update(item.keys())\n",
    "                \n",
    "                # List of columns to check and create if not exist\n",
    "                cols_to_check = [\n",
    "                    'properties.utilization.aggregates[0].value', \n",
    "                    'properties.utilization.aggregates[1].value', \n",
    "                    'properties.utilization.aggregates[2].value', \n",
    "                    'properties.utilization.aggregates[0].valueUnit',\n",
    "                    'properties.utilization.aggregates[1].valueUnit', \n",
    "                    'properties.utilization.aggregates[2].valueUnit', \n",
    "                    'properties.utilization.aggregates[0].grain', \n",
    "                    'properties.utilization.aggregates[1].grain',\n",
    "                    'properties.utilization.aggregates[2].grain', \n",
    "                    'properties.utilization.aggregates[0].grainUnit', \n",
    "                    'properties.utilization.aggregates[1].grainUnit', \n",
    "                    'properties.utilization.aggregates[2].grainUnit',\n",
    "                    'properties.billingPlan',\n",
    "                    'properties.utilization.trend',\n",
    "                    'properties.appliedScopeProperties.subscriptionId',\n",
    "                    'properties.appliedScopeProperties.resourceGroupId',\n",
    "                    'properties.appliedScopeProperties.TenantId',\n",
    "                    'properties.appliedScopeProperties.ManagementGroupId',\n",
    "                    'properties.appliedScopeProperties.displayName'\n",
    "                ]\n",
    "\n",
    "                # Create columns with default values if they don't exist\n",
    "                for col_name in cols_to_check:\n",
    "                    if col_name not in fieldnames:\n",
    "                        fieldnames.add(col_name)\n",
    "                        # Set default values based on the column name\n",
    "                       \n",
    "                        if col_name in ['properties.utilization.aggregates[0].value', 'properties.utilization.aggregates[1].value', 'properties.utilization.aggregates[2].value']:\n",
    "                            default_value = 0\n",
    "                        elif col_name in ['properties.utilization.aggregates[0].valueUnit', 'properties.utilization.aggregates[1].valueUnit', 'properties.utilization.aggregates[2].valueUnit']:\n",
    "                            default_value = 'percentage'\n",
    "                        elif col_name in ['properties.utilization.aggregates[0].grain', 'properties.utilization.aggregates[1].grain', 'properties.utilization.aggregates[2].grain']:\n",
    "                            default_value = '0'\n",
    "                        elif col_name in ['properties.utilization.aggregates[0].grainUnit', 'properties.utilization.aggregates[1].grainUnit', 'properties.utilization.aggregates[2].grainUnit']:\n",
    "                            default_value = 'days'\n",
    "                        elif col_name in ['properties.billingPlan']:\n",
    "                            default_value = 'N/A'\n",
    "                        elif col_name in ['properties.utilization.trend']:\n",
    "                            default_value = 'N/A'     \n",
    "                        else:\n",
    "                            default_value = None\n",
    "                        \n",
    "                        # Update each record with the default value only if the value is NULL or empty\n",
    "                        for item in flattened_data:\n",
    "                                item[col_name] = default_value  # Set the default value\n",
    "                # Write CSV\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=sorted(fieldnames))  # Use keys as header\n",
    "                writer.writeheader()\n",
    "                writer.writerows(flattened_data)\n",
    "            \n",
    "            print(f\"CSV file successfully created at {csv_filename}.\")\n",
    "        else:\n",
    "            print(\"No reservation data to write to CSV file.\")\n",
    "    else:\n",
    "        print(\"No reservations found.\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3eebf-9015-4bfa-8ec0-e82d7d048aa2",
   "metadata": {},
   "source": [
    "# Get Azure Saving Plans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee47aa4-2738-45e2-9f06-07a7cd500dff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# API version for Azure Resource Manager\n",
    "api_version = '2022-11-01'  # Alternative: try '2021-10-01'\n",
    "\n",
    "# Header with Bearer Token\n",
    "#headers = {\n",
    "    #'Authorization': f'Bearer {token}',\n",
    "    #'Content-Type': 'application/json'\n",
    "#}\n",
    "\n",
    "# Target path for CSV file\n",
    "destination_path = SavingPlansPath\n",
    "csv_filename = os.path.join(destination_path, f'Saving-Plan-{microsoft_tenant_id}.csv')\n",
    "\n",
    "# Check if target directory exists, if not, create it\n",
    "if not os.path.exists(destination_path):\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# URL for Azure Saving\n",
    "savingplan_url = f'https://management.azure.com/providers/Microsoft.BillingBenefits/savingsPlans?api-version={api_version}'\n",
    "\n",
    "# Function to flatten nested JSON objects\n",
    "def flatten_json(nested_json, parent_key='', sep='.'):\n",
    "    flattened = {}\n",
    "    for key, value in nested_json.items():\n",
    "        new_key = f'{parent_key}{sep}{key}' if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, sep=sep))\n",
    "        elif isinstance(value, list):\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    flattened.update(flatten_json(item, f'{new_key}[{i}]', sep=sep))\n",
    "                else:\n",
    "                    flattened[f'{new_key}[{i}]'] = item\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "# Fetch saving plans and write directly to CSV file\n",
    "try:\n",
    "    savingplans_response = requests.get(savingplan_url, headers=get_headers()) # NEW get_headers() #\n",
    "    \n",
    "    print(\"Status code:\", savingplans_response.status_code)  # Print status code\n",
    "    if savingplans_response.status_code == 200:\n",
    "        savingplan = savingplans_response.json()  # Store entire response as JSON object\n",
    "        print(\"Saving Plan successfully retrieved.\")\n",
    "        \n",
    "        # Extract data and write directly to CSV file\n",
    "        if 'value' in savingplan:  # Assumption: reservations are in the \"value\" field\n",
    "            data = savingplan['value']\n",
    "            \n",
    "            # Check if there are saving plans\n",
    "            if data:\n",
    "                # Prepare flat list\n",
    "                flattened_data = [flatten_json(item) for item in data]\n",
    "                \n",
    "                # Add Filename column\n",
    "                for item in flattened_data:\n",
    "                    item['Filename'] = os.path.basename(csv_filename)  # Only filename without path\n",
    "                \n",
    "                # CSV header: all unique keys from flat data\n",
    "                fieldnames = set()\n",
    "                for item in flattened_data:\n",
    "                    fieldnames.update(item.keys())\n",
    "                \n",
    "                # List of columns to check and create if not exist\n",
    "                cols_to_check = [\n",
    "                    'Filename',\n",
    "                    'id',\n",
    "                    'name',\n",
    "                    'properties.appliedScopeProperties.displayName',\n",
    "                    'properties.appliedScopeProperties.subscriptionId',\n",
    "                    'properties.appliedScopeProperties.resourceGroupId',\n",
    "                    'properties.appliedScopeProperties.TenantId',\n",
    "                    'properties.appliedScopeProperties.ManagementGroupId',\n",
    "                    'properties.appliedScopeType',\n",
    "                    'properties.benefitStartTime',\n",
    "                    'properties.billingAccountId',\n",
    "                    'properties.billingPlan',\n",
    "                    'properties.billingProfileId',\n",
    "                    'properties.billingScopeId',\n",
    "                    'properties.commitment.amount',\n",
    "                    'properties.commitment.currencyCode',\n",
    "                    'properties.commitment.grain',\n",
    "                    'properties.customerId',\n",
    "                    'properties.displayName',\n",
    "                    'properties.displayProvisioningState',\n",
    "                    'properties.effectiveDateTime',\n",
    "                    'properties.expiryDateTime',\n",
    "                    'properties.provisioningState',\n",
    "                    'properties.purchaseDateTime',\n",
    "                    'properties.renew',\n",
    "                    'properties.term',\n",
    "                    'properties.userFriendlyAppliedScopeType',\n",
    "                    'properties.utilization.aggregates[0].grain',\n",
    "                    'properties.utilization.aggregates[0].grainUnit',\n",
    "                    'properties.utilization.aggregates[0].value',\n",
    "                    'properties.utilization.aggregates[0].valueUnit',\n",
    "                    'properties.utilization.aggregates[1].grain',\n",
    "                    'properties.utilization.aggregates[1].grainUnit',\n",
    "                    'properties.utilization.aggregates[1].value',\n",
    "                    'properties.utilization.aggregates[1].valueUnit',\n",
    "                    'properties.utilization.aggregates[2].grain',\n",
    "                    'properties.utilization.aggregates[2].grainUnit',\n",
    "                    'properties.utilization.aggregates[2].value',\n",
    "                    'properties.utilization.aggregates[2].valueUnit',\n",
    "                    'properties.utilization.trend',\n",
    "                    'sku.name',\n",
    "                    'type'\n",
    "                ]\n",
    "\n",
    "                # Create columns with default values if they don't exist\n",
    "                for col_name in cols_to_check:\n",
    "                    if col_name not in fieldnames:\n",
    "                        fieldnames.add(col_name)\n",
    "                        # Set default values based on the column name\n",
    "                       \n",
    "                        if col_name.startswith('properties.utilization.aggregates'):\n",
    "                            if 'value' in col_name:\n",
    "                                default_value = 0\n",
    "                            elif 'valueUnit' in col_name:\n",
    "                                default_value = 'percentage'\n",
    "                            elif 'grain' in col_name:\n",
    "                                default_value = '0'\n",
    "                            elif 'grainUnit' in col_name:\n",
    "                                default_value = 'days'\n",
    "                        elif col_name == 'properties.billingPlan' or col_name == 'properties.utilization.trend':\n",
    "                            default_value = 'N/A'\n",
    "                        else:\n",
    "                            default_value = None\n",
    "                        \n",
    "                        # Update each record with the default value only if the value is NULL or empty\n",
    "                        for item in flattened_data:\n",
    "                            item[col_name] = default_value\n",
    "                \n",
    "                # Print schema\n",
    "                print(\"Schema of CSV file (column names):\")\n",
    "                for index, fieldname in enumerate(sorted(fieldnames), start=1):\n",
    "                    print(f\"{index}. {fieldname}\")\n",
    "                \n",
    "                # Create CSV file and write data\n",
    "                with open(csv_filename, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "                    writer = csv.DictWriter(csv_file, fieldnames=sorted(fieldnames))  # Use keys as header\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(flattened_data)\n",
    "                \n",
    "                print(f\"CSV file successfully created at {csv_filename}.\")\n",
    "            else:\n",
    "                print(\"No reservation data to write to CSV file.\")\n",
    "        else:\n",
    "            print(\"The JSON response does not contain a 'value' field.\")\n",
    "    \n",
    "    elif savingplans_response.status_code == 403:\n",
    "        print(\"No Saving Plans available or no access.\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Error retrieving Saving Plans: {savingplans_response.status_code}\")\n",
    "        print(\"Error message:\", savingplans_response.text)  # Show error message\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52423c3d-c794-4620-8775-95e455326865",
   "metadata": {},
   "source": [
    "# Get Azure Resource Tags and ResourceGroup Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133c402-3c62-4dc4-904c-f7aa7635b8af",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# API version for Azure Resource Manager\n",
    "api_version = '2021-04-01'\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Header with Bearer Token (insert token here)\n",
    "#headers = {\n",
    "    #'Authorization': f'Bearer {token}',\n",
    "    #'Content-Type': 'application/json'\n",
    "#}\n",
    "\n",
    "def perform_request_with_retries(url, method=\"GET\", retries=3, backoff_factor=2, max_retry_after=60, **kwargs):\n",
    "    \"\"\"Perform HTTP request with automatic retries on rate limit errors (429).\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.request(method, url, **kwargs)\n",
    "            if response.status_code == 429:\n",
    "                retry_after = int(response.headers.get(\"Retry-After\", backoff_factor))\n",
    "                retry_after = min(retry_after, max_retry_after)  # Max 1 minute wait\n",
    "                logger.warning(f\"Rate limit reached. Retrying in {retry_after} seconds (attempt {attempt + 1} of {retries})...\")\n",
    "                time.sleep(retry_after)\n",
    "            else:\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Error making request to {url}: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                sleep_time = backoff_factor * (2 ** attempt)\n",
    "                logger.info(f\"Waiting {sleep_time} seconds before retry...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                logger.error(\"Maximum number of retries reached.\")\n",
    "                raise\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_resource_groups(subscription_id):\n",
    "    \"\"\"Fetch all resource groups for a subscription.\"\"\"\n",
    "    resource_groups_url = f'https://management.azure.com/subscriptions/{subscription_id}/resourcegroups?api-version={api_version}'\n",
    "    response = perform_request_with_retries(resource_groups_url, headers=get_headers(), timeout=10) # NEW get_headers() #\n",
    "    if response:\n",
    "        return response.json().get('value', [])\n",
    "    return []\n",
    "\n",
    "def get_rg_tags(subscription_id, resource_group_name):\n",
    "    \"\"\"Fetch tags for a resource group.\"\"\"\n",
    "    rg_tags_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourcegroups/{resource_group_name}?api-version={api_version}\"\n",
    "    response = perform_request_with_retries(rg_tags_url, headers=get_headers(), timeout=10) # NEW get_headers() #\n",
    "    if response:\n",
    "        return response.json().get('tags', {})\n",
    "    return {}\n",
    "\n",
    "def write_to_csv(subscription_id, csv_data):\n",
    "    \"\"\"Write CSV data to file.\"\"\"\n",
    "    destination_path = \"/lakehouse/default/Files/azure/resource-import-tags\"\n",
    "    os.makedirs(destination_path, exist_ok=True)\n",
    "    csv_filename = os.path.join(destination_path, f'ResourceTags-{subscription_id}.csv')\n",
    "\n",
    "    if csv_data:\n",
    "        try:\n",
    "            with open(csv_filename, mode='w', newline='') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=['ResourceGroup', 'ResourceId', 'ResourceName', 'TagKey', 'TagValue'], delimiter=';')\n",
    "                writer.writeheader()\n",
    "                writer.writerows(csv_data)\n",
    "            logger.info(f\"CSV file successfully created at {csv_filename}.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing CSV file for subscription {subscription_id}: {e}\")\n",
    "    else:\n",
    "        logger.info(f\"No data to write to CSV file for subscription {subscription_id}.\")\n",
    "\n",
    "def process_subscription(subscription):\n",
    "    subscription_id = subscription['subscriptionId']\n",
    "    logger.info(f\"Processing Subscription ID: {subscription_id}, Name: {subscription['displayName']}\")\n",
    "\n",
    "    csv_data = []\n",
    "    resource_groups = get_resource_groups(subscription_id)\n",
    "\n",
    "    if not resource_groups:\n",
    "        logger.info(f\"No resource groups found for subscription {subscription_id}.\")\n",
    "        return\n",
    "\n",
    "    for resource_group in resource_groups:\n",
    "        logger.info(f\"Resource Group: {resource_group['name']}\")\n",
    "\n",
    "        rg_tags = get_rg_tags(subscription_id, resource_group['name'])\n",
    "\n",
    "        if not rg_tags:\n",
    "            logger.info(f\"No tags found for resource group {resource_group['name']}.\")\n",
    "        else:\n",
    "            for key, value in rg_tags.items():\n",
    "                csv_data.append({\n",
    "                    'ResourceGroup': resource_group['name'],\n",
    "                    'ResourceId': resource_group['id'],\n",
    "                    'ResourceName': 'NO RESOURCE',\n",
    "                    'TagKey': key,\n",
    "                    'TagValue': value\n",
    "                })\n",
    "                logger.info(f\"Tag added for resource group: {key} = {value}\")\n",
    "\n",
    "    write_to_csv(subscription_id, csv_data)\n",
    "\n",
    "def get_subscriptions():\n",
    "    \"\"\"Fetch all subscriptions.\"\"\"\n",
    "    subscriptions_url = f'https://management.azure.com/subscriptions?api-version={api_version}'\n",
    "    response = perform_request_with_retries(subscriptions_url, headers=get_headers(), timeout=10) # NEW get_headers() #\n",
    "    if response:\n",
    "        return response.json().get('value', [])\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    subscriptions = get_subscriptions()\n",
    "\n",
    "    if not subscriptions:\n",
    "        logger.info(\"No subscriptions found.\")\n",
    "        return\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(process_subscription, sub): sub for sub in subscriptions}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            subscription = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing subscription {subscription['subscriptionId']}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca922a85-38fd-4b9b-87b0-3d275613e80a",
   "metadata": {},
   "source": [
    "# Get Azure Subscription Tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff3433-9bd8-4ecd-a32d-9b52365031eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# API version for Azure Resource Manager\n",
    "api_version = '2021-04-01'\n",
    "\n",
    "# Header with Bearer Token\n",
    "#headers = {\n",
    "    #'Authorization': f'Bearer {token}',\n",
    "    #'Content-Type': 'application/json'\n",
    "#}\n",
    "\n",
    "# Target path for CSV file (e.g., Data Lake House path)\n",
    "destination_path = \"/lakehouse/default/Files/azure/subscription-tags\"\n",
    "csv_filename = os.path.join(destination_path, f'SubscriptionTags-{microsoft_tenant_id}.csv')\n",
    "\n",
    "# Check if path exists, if not, create it\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "\n",
    "# Fetch all subscriptions\n",
    "subscriptions_url = f'https://management.azure.com/subscriptions?api-version={api_version}'\n",
    "subscriptions_response = requests.get(subscriptions_url, headers=get_headers()) # NEW get_headers() #\n",
    "\n",
    "# Store CSV data for all subscriptions\n",
    "csv_data = []\n",
    "\n",
    "if subscriptions_response.status_code == 200:\n",
    "    subscriptions = subscriptions_response.json()['value']\n",
    "    \n",
    "    if subscriptions:\n",
    "        for subscription in subscriptions:\n",
    "            subscription_id = subscription['subscriptionId']\n",
    "\n",
    "            # Fetch tags for subscription\n",
    "            tags_url = f\"https://management.azure.com/subscriptions/{subscription_id}/providers/Microsoft.Resources/tags/default?api-version={api_version}\"\n",
    "            tags_response = requests.get(tags_url, headers=get_headers()) # NEW get_headers() #\n",
    "            \n",
    "            if tags_response.status_code == 200 and 'tags' in tags_response.json()['properties']:\n",
    "                tags = tags_response.json()['properties']['tags']\n",
    "\n",
    "                # Add tags to CSV data list\n",
    "                for key, value in tags.items():\n",
    "                    csv_data.append({\n",
    "                        'Id': subscription_id,\n",
    "                        'TagKey': key,\n",
    "                        'Value': value\n",
    "                    })\n",
    "\n",
    "# Check if data is available to write\n",
    "if csv_data:\n",
    "    # Create CSV file directly in target path\n",
    "    with open(csv_filename, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['Id', 'TagKey', 'Value'], delimiter=',')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_data)\n",
    "    print(f\"CSV file successfully created at {csv_filename}.\")\n",
    "else:\n",
    "    print(\"No tags found, CSV file was not created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79c50d-098f-4c91-80a4-9f9603831f91",
   "metadata": {},
   "source": [
    "# Get Access Token for Graph API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f694f68-3dc8-4b04-bba2-8a1cc9d2518f",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# The App ID, Client Secret and Tenant ID from Key Vault or environment\n",
    "# app_client_id = mssparkutils.credentials.getSecret(f\"https://{key_vault_name}.vault.azure.net/\", kvclientid_1)\n",
    "# app_client_secret = mssparkutils.credentials.getSecret(f\"https://{key_vault_name}.vault.azure.net/\", kvclientkey_1)\n",
    "# microsoft_tenant_id = mssparkutils.credentials.getSecret(f\"https://{key_vault_name}.vault.azure.net/\", kvtenantid_1)\n",
    "\n",
    "# URL to fetch Bearer Token for Microsoft Graph API\n",
    "uri = f\"https://login.microsoftonline.com/{microsoft_tenant_id}/oauth2/v2.0/token\"\n",
    "\n",
    "# Required parameters to fetch token\n",
    "data = {\n",
    "    'client_id': app_client_id,\n",
    "    'client_secret': app_client_secret,\n",
    "    'grant_type': 'client_credentials',\n",
    "    'scope': 'https://graph.microsoft.com/.default'  # Scopes for Microsoft Graph API\n",
    "}\n",
    "\n",
    "# API call to fetch Bearer Token\n",
    "response = requests.post(uri, data=data)\n",
    "response = response.json()\n",
    "\n",
    "# Set variables from API response: access_token and token_type\n",
    "ms_token_type = response.get('token_type', None)\n",
    "token = response.get('access_token', None)\n",
    "\n",
    "if token:\n",
    "    print(\"Token successfully retrieved!\")\n",
    "else:\n",
    "    print(\"Error retrieving token:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abc70da-9aef-4c35-a6f8-a3ad408b6a68",
   "metadata": {},
   "source": [
    "# Query Microsoft Licenses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75535159-1cf9-41bd-a367-fc9ebb282332",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV mapping\n",
    "#mapping_path = \"abfss://2044bd38-1f90-4cd0-b1b1-5f7a7d26e734@onelake.dfs.fabric.microsoft.com/44b967f6-ac81-40a7-bc53-128091aba32b/Files/m365/product-mapping/sku_mapping.csv\"\n",
    "mapping_path = sku_mapping_path\n",
    "mapping_df = pd.read_csv(mapping_path)\n",
    "sku_mapping = dict(zip(mapping_df['skuPartNumber'], mapping_df['DisplayName']))\n",
    "\n",
    "# Authentication and setup\n",
    "headers = {\n",
    "    'Authorization': f\"Bearer {token}\",\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Fetch licenses\n",
    "sku_url = 'https://graph.microsoft.com/v1.0/subscribedSkus'\n",
    "\n",
    "# v1.0 API\n",
    "response = requests.get(sku_url, headers=headers)\n",
    "skus = response.json().get('value', [])\n",
    "\n",
    "#display(skus)\n",
    "\n",
    "# Extract data and add display names\n",
    "license_data = []\n",
    "for sku in skus:\n",
    "    sku_part_number = sku.get('skuPartNumber', '')\n",
    "    product_name = sku_mapping.get(sku_part_number, sku_part_number)  # Mapping with fallback\n",
    "    license_data.append({\n",
    "        #'SkuId': sku.get('skuId', ''),\n",
    "        'ProductName': product_name,\n",
    "        'AvailableLicenses': sku.get('prepaidUnits', {}).get('enabled', 0),\n",
    "        'UsedLicences': sku.get('consumedUnits', 0),\n",
    "        'StillAvailableLicenses': sku.get('prepaidUnits', {}).get('enabled', 0) - sku.get('consumedUnits', 0),\n",
    "        'Tenant_Id': microsoft_tenant_id  # Add tenant ID to each entry\n",
    "    })\n",
    "\n",
    "# Create DataFrame & display\n",
    "df = pd.DataFrame(license_data)\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "df_sparkms_tenant_licenses = spark.createDataFrame(df)\n",
    "\n",
    "#display(df_sparkms_tenant_licenses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5f495-4a7e-415a-8fed-12b57575d6f0",
   "metadata": {},
   "source": [
    "# Create Staging Table Microsoft Tenant Licenses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e8c84-5f05-4658-8c55-558e307dfe73",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# Path to Delta Table\n",
    "delta_table_path_stagingdimazmstenantlicenses = \"Tables/StagingdimAzMSTenantLicences\"\n",
    "\n",
    "# Write to Delta Table\n",
    "df_sparkms_tenant_licenses.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_stagingdimazmstenantlicenses)\n",
    "\n",
    "print(\"Data successfully written to Delta Table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a73145-9b6a-4c2f-8ce4-a98f56f63fe7",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# === Embedded license mapping ===\n",
    "mapping_json = {\n",
    "    \"ENTERPRISEPACK\": {\n",
    "        \"name\": \"Microsoft 365 E3\",\n",
    "        \"includes\": [\"Office_Apps\", \"Exchange_Online\", \"SharePoint_Online\", \"Teams\",\n",
    "                     \"Intune\", \"Azure_AD_P1\", \"PowerApps_Standard\", \"PowerAutomate_Standard\"]\n",
    "    },\n",
    "    \"E5\": {\n",
    "        \"name\": \"Microsoft 365 E5\",\n",
    "        \"includes\": [\"Office_Apps\", \"Exchange_Online\", \"SharePoint_Online\", \"Teams\",\n",
    "                     \"Intune\", \"Azure_AD_P2\", \"Defender\", \"PowerApps_Standard\",\n",
    "                     \"PowerAutomate_Standard\", \"POWER_BI_PRO\"]\n",
    "    },\n",
    "    \"BUSINESS_PREMIUM\": {\n",
    "        \"name\": \"Microsoft 365 Business Premium\",\n",
    "        \"includes\": [\"Office_Apps\", \"Exchange_Online\", \"SharePoint_Online\", \"Teams\",\n",
    "                     \"PowerApps_Standard\", \"PowerAutomate_Standard\"]\n",
    "    },\n",
    "    \"POWER_BI_PRO\": {\n",
    "        \"name\": \"Power BI Pro\",\n",
    "        \"includes\": [\"POWER_BI_PRO\"]\n",
    "    },\n",
    "    \"PBI_PREMIUM_PER_USER\": {\n",
    "        \"name\": \"Power BI Premium per User\",\n",
    "        \"includes\": [\"POWER_BI_PRO\", \"PowerBI_Premium\"]\n",
    "    },\n",
    "    \"POWERAPPS_PER_USER\": {\n",
    "        \"name\": \"Power Apps per User\",\n",
    "        \"includes\": [\"PowerApps_Premium\"]\n",
    "    },\n",
    "    \"O365_BUSINESS_PREMIUM\": {\n",
    "        \"name\": \"Office 365 Business Premium\",\n",
    "        \"includes\": [\"Office_Apps\", \"Exchange_Online\", \"SharePoint_Online\", \"Teams\",\n",
    "                     \"PowerApps_Standard\", \"PowerAutomate_Standard\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Load CSV with caching ===\n",
    "mapping_path = (\n",
    "    \"abfss://2044bd38-1f90-4cd0-b1b1-5f7a7d26e734@onelake.dfs.fabric.microsoft.com/\"\n",
    "    \"44b967f6-ac81-40a7-bc53-128091aba32b/Files/m365/product-mapping/sku_mapping.csv\"\n",
    ")\n",
    "mapping_df = pd.read_csv(mapping_path)\n",
    "sku_mapping = dict(zip(mapping_df[\"skuPartNumber\"], mapping_df[\"DisplayName\"]))\n",
    "\n",
    "# === Prepare auth headers ===\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'Authorization': f\"Bearer {token}\",\n",
    "    'Content-Type': 'application/json'\n",
    "})\n",
    "\n",
    "# === Query users (load all pages) ===\n",
    "users = []\n",
    "users_url = 'https://graph.microsoft.com/v1.0/users?$select=id,displayName,userPrincipalName,mail,givenName,surname,companyName,department,userType'\n",
    "while users_url:\n",
    "    r = session.get(users_url)\n",
    "    data = r.json()\n",
    "    users.extend(data.get('value', []))\n",
    "    users_url = data.get('@odata.nextLink')\n",
    "\n",
    "# === Functions ===\n",
    "def get_license_details(user):\n",
    "    \"\"\" Fetch license details for a user with error handling \"\"\"\n",
    "    uid = user['id']\n",
    "    url = f\"https://graph.microsoft.com/v1.0/users/{uid}/licenseDetails\"\n",
    "    try:\n",
    "        resp = session.get(url, timeout=10).json()\n",
    "    except Exception:\n",
    "        resp = {}\n",
    "\n",
    "    licenses = resp.get('value', [])\n",
    "    results = []\n",
    "\n",
    "    if not licenses:\n",
    "        results.append({\n",
    "            \"sku\": \"None\",\n",
    "            \"name\": \"No License\",\n",
    "            \"features\": set()\n",
    "        })\n",
    "        return uid, results\n",
    "\n",
    "    for lic in licenses:\n",
    "        sku = lic.get('skuPartNumber', 'Unknown')\n",
    "        readable_name = sku_mapping.get(sku, sku)\n",
    "        features = set(mapping_json.get(sku, {}).get(\"includes\", []))\n",
    "        results.append({\n",
    "            \"sku\": sku,\n",
    "            \"name\": readable_name,\n",
    "            \"features\": features\n",
    "        })\n",
    "    return uid, results\n",
    "\n",
    "\n",
    "# === Load license details in parallel (much faster) ===\n",
    "user_license_map = {}\n",
    "with ThreadPoolExecutor(max_workers=20) as executor:  # 20 threads = ideal for Graph API\n",
    "    futures = {executor.submit(get_license_details, u): u for u in users}\n",
    "    for f in as_completed(futures):\n",
    "        uid, licenses = f.result()\n",
    "        user_license_map[uid] = licenses\n",
    "\n",
    "# === Check over-licensing ===\n",
    "user_license_data = []\n",
    "for user in users:\n",
    "    uid = user['id']\n",
    "    display_name = user.get('displayName', '')\n",
    "    upn = user.get('userPrincipalName', '')\n",
    "    mail = user.get('mail') or 'N/A'\n",
    "    given_name = user.get('givenName') or 'N/A'\n",
    "    surname = user.get('surname', '')\n",
    "    companyName = user.get('companyName') or 'N/A'\n",
    "    department = user.get('department') or 'N/A'\n",
    "    userType = user.get('userType', '')\n",
    "\n",
    "    licenses = user_license_map.get(uid, [])\n",
    "    covered_by = {}\n",
    "\n",
    "    # Feature comparison accelerated by pre-calculation\n",
    "    for i, lic in enumerate(licenses):\n",
    "        features_i = lic[\"features\"]\n",
    "        if not features_i:\n",
    "            continue\n",
    "        for j, other in enumerate(licenses):\n",
    "            if i == j:\n",
    "                continue\n",
    "            if features_i.issubset(other[\"features\"]) and features_i != other[\"features\"]:\n",
    "                covered_by[lic[\"name\"]] = other[\"name\"]\n",
    "\n",
    "    for lic in licenses:\n",
    "        overlic = covered_by.get(lic[\"name\"], \"None\")\n",
    "        if overlic not in sku_mapping.values():\n",
    "            overlic = \"None\"\n",
    "\n",
    "        user_license_data.append({\n",
    "            'ID': uid,\n",
    "            'Name': display_name,\n",
    "            'UPN': upn,\n",
    "            'License': lic[\"name\"],\n",
    "            'SKU_Code': lic[\"sku\"],\n",
    "            'Tenant_Id': microsoft_tenant_id,\n",
    "            'E-Mail': mail,\n",
    "            'GivenName': given_name,\n",
    "            'SurName': surname,\n",
    "            'CompanyName': companyName,\n",
    "            'Department': department,\n",
    "            'UserType': userType,\n",
    "            'OverLicensing': overlic\n",
    "        })\n",
    "\n",
    "# === Convert to DataFrame ===\n",
    "df = pd.DataFrame(user_license_data)\n",
    "df_sparkms_user_licenses = spark.createDataFrame(df)\n",
    "\n",
    "#display(df_sparkms_user_licenses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53ffae-7041-4d58-b858-b84f1c2211f8",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [],
   "source": [
    "# Path to Delta Table\n",
    "delta_table_path_stagingdimazmsuserlicenses = \"Tables/StagingdimAzMSUserLicenses\"\n",
    "\n",
    "# Write to Delta Table\n",
    "df_sparkms_user_licenses.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path_stagingdimazmsuserlicenses)\n",
    "\n",
    "print(\"Data successfully written to Delta Table.\")\n"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "7bd7dae0-986b-4b7c-89ad-89ef3374fc3b",
    "default_lakehouse_name": "lakehouse01",
    "default_lakehouse_workspace_id": "17d33f5a-c05d-4bc3-87b3-fc14fa12dad9",
    "known_lakehouses": [
     {
      "id": "7bd7dae0-986b-4b7c-89ad-89ef3374fc3b"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
